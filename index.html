<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>陈佳鑫 (Jiaxin Chen)</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>陈佳鑫 (Jiaxin Chen)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://dr-jiaxin-chen.github.io/page/"><img src="profile_photo.png" alt="alt text" width="120px" /></a>&nbsp;</td>
<td align="left"><p>副教授<br />
<a href="http://scse.buaa.edu.cn/">计算机学院</a>, <br />
<a href="https://www.buaa.edu.cn/">北京航空航天大学</a>, <br />
北京 100191, 中国 <br />
邮箱: jiaxinchen@buaa.edu.cn <br />
<br />
<a href="http://shi.buaa.edu.cn/chenjiaxin/zh_CN/index.htm">[简历]</a> <a href="http://shi.buaa.edu.cn/chenjiaxin/zh_CN/index.htm">[Google Scholar]</a> 
  <a href="https://github.com/Dr-Jiaxin-Chen">[GitHub]<a href="http://shi.buaa.edu.cn/chenjiaxin/en/index.htm">[English Page]</a></p>
</td></tr></table>
<h2>关于我</h2>
<p>
陈佳鑫，男，博士。先后于2009年、2012年和2017年获北航应用数学专业学士学位、数学专业硕士学位及计算机应用技术专业博士学位。2017至2018年间在纽约大学阿布扎比分校从事博士后研究工作。2018至2021年在起源人工智能研究院担任研究科学家，现为北京航空航天大学计算机学院副教授/硕士生导师。目前主要研究方向包括计算机视觉（智能视频分析、图像处理、三维视觉分析等）、机器学习（度量学习、哈希学习等）。作为核心成员参与多项国家自然科学基金项目、港澳台合作项目等。相关研究成果已发表论文20余篇，包括IEEE Transactions on Image Processing (TIP)、IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)等顶级期刊；CVPR、ICCV、ECCV等计算机视觉领域顶级会议。</p>
<p>研究方向: 计算机视觉；模式识别；机器学习。</p>
<h2>学术论文</h2>
<h3>会议</h3>
<h4>2022</h4>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.03569">Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement</a> <br />
B. Li, <b>J. Chen*(*：通讯作者)</b>, D. Zhang, X. Bao and D. Huang <br />
<i>International Joint Conference on Artificial Intelligence (IJCAI), 2022. <a href="https://arxiv.org/pdf/2205.03569.pdf">[PDF]</a></i></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_CAT-Det_Contrastively_Augmented_Transformer_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.html">CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection</a> <br />
Y. Zhang, <b>J. Chen</b> and D. Huang <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_CAT-Det_Contrastively_Augmented_Transformer_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.html">[PDF]</a></i></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Target-Relevant_Knowledge_Preservation_for_Multi-Source_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection</a> <br />
J. Wu, <b>J. Chen*(*：通讯作者)</b>, M. He, Y. Wang, B. Li, B. Ma, W. Gan, W. Wu, Y. Wang and D. Huang <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Target-Relevant_Knowledge_Preservation_for_Multi-Source_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">[PDF]</a></i></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Entropy-Based_Active_Learning_for_Object_Detection_With_Progressive_Diversity_Constraint_CVPR_2022_paper.pdf">Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint</a> <br />
J. Wu, <b>J. Chen</b> and D. Huang <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Entropy-Based_Active_Learning_for_Object_Detection_With_Progressive_Diversity_Constraint_CVPR_2022_paper.pdf">[PDF]</a></i></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.10415v2.pdf">UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery</a> <br />
Y. Huang, <b>J. Chen</b> and D. Huang <br />
<i>AAAI Conference on Artificial Intelligencen (AAAI), 2022. <a href="https://arxiv.org/pdf/2112.10415v2.pdf">[PDF]</a></i></p>
</li>
</ul>
<h4>2021</h4>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9711034/">PR-GCN: A Deep Graph Convolutional Network with Point Refinement for 6D Pose Estimation</a> <br />
G. Zhou, H. Wang, <b>J. Chen</b> and D. Huang <br />
<i> IEEE International Conference on Computer Vision (ICCV), 2021. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9711034&tag=1">[PDF]</a></i></p>
</li>
</ul>
<h4>2020</h4>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9157770">Auto-Encoding Twin-Bottleneck Hashing</a> <br />
Y. Shen#, J. Qin#, <b>J. Chen# (#: 共同一作)</b>, M. Yu, L. Liu, F. Zhu, F. Shen and L. Shao <br />
<i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9157770">[PDF]</a></i></p>
</li>
<li><p><a href="http://dl.acm.org/doi/pdf/10.1145/3394171.3413979">Deep Local Binary Coding for Person Re- Identification by Delving into the Details</a> <br />
<b>J. Chen</b>, J. Qin, Y. Yan, L. Huang, L. Liu, F. Zhu and L. Shao <br />
<i> ACM Multimedia (MM), 2020. <a href="http://dl.acm.org/doi/pdf/10.1145/3394171.3413979">[PDF]</a></i></p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-030-58555-6_7">Learning Attentive and Hierarchical Representations for 3D Shape Recognition</a> <br />
<b>J. Chen</b>, J. Qin, Y. Shen, L. Liu, F. Zhu and L. Shao <br />
<i> European Conference on Computer Vision (ECCV), 2020. <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-58555-6.pdf">[PDF]</a></i></p>
</li>
</ul>
<h3>期刊</h3>
<h4>2022</h4>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9817378">Video Person Re-identification Using Attribute-enhanced Features</a> <br />
T. Chai, Z. Chen, A. Li, <b>J. Chen<\b>, X. Mei, Y. Wang <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2022, doi:10.1109/TCSVT.2022.3189027. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9817378">[PDF]</a></i></p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0097849322001261?dgcid=coauthor">SHREC’22 Track: Sketch-Based 3D Shape Retrieval in the Wild</a> <br />
J. Qin, S. Yuan, <b>J. Chen<\b>, et al. <br />
<i>Computers & Graphics (CAG), vol. 107, pp: 104-115, 2022. <a href="https://www.sciencedirect.com/science/article/pii/S0097849322001261/pdfft?crasolve=1&r=74266d514d169440&ts=1661788065507&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&iv=3fc0b0a3a58d93e36ea7363909204a6f&token=66666439613464316438616362323664646335626333336564336534393762316361663330313366306230666261353965346235646339626334313833613666613539353a386232356437633631363839356363643637636365353663&text=c323c076bac6b0667c45ca5920ba551e1b1ce05c449295348e0dc5deecb8fa622ecf23f1bba40b4197293a09f17854064a8bc6df3d353b5eac07dcc12cbd149eb04e184e85f742636298f02078eabfcc5ec9a06686909ccea881b9be42fbd05ad9aafec59164920d862c3cb2ec0611c6e7a275900d1a3a29275b71a8118e0323cb43a1ea934b670f7a596434ffa5e00483c8f5968997d936832c3a9a4a5e09da9c9550243627c892bae370d2846de22f096c5b34bd289973d26e9a03a8182808de595d99ead0ca370311438a687de84d28e2d90b737ea2f07e36114f04bf302b961cac7fd175150c82939a11f09f94dcadc1fe3e103f854434e96dadf8b6a690965dd38e93e0fe3d9c5245866755d235a7561de51e48c594dbd2a6337642a44a9107dffc634af57de96a2d507569a35b&original=3f6d64353d6231323637643939366365613465636562396365623037663237333863643839267069643d312d73322e302d53303039373834393332323030313236312d6d61696e2e706466">[PDF]</a></i></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
<br>Page generated 2022-08-01, by <a href="https://dr-jiaxin-chen.github.io/page/">Jiaxin Chen</a>.
</div>
</div>
</div>
</body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>陈佳鑫 (Jiaxin Chen)</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>陈佳鑫 (Jiaxin Chen)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://dr-jiaxin-chen.github.io/page/"><img src="profile_photo.png" alt="alt text" width="120px" /></a>&nbsp;</td>
<td align="left"><p>副教授<br />
<a href="http://scse.buaa.edu.cn/">计算机学院</a>, <br />
<a href="https://www.buaa.edu.cn/">北京航空航天大学</a>, <br />
北京 100191, 中国 <br />
邮箱: jiaxinchen@buaa.edu.cn <br />
<br />
<a href="http://shi.buaa.edu.cn/chenjiaxin/zh_CN/index.htm">[简历]</a> <a href="http://shi.buaa.edu.cn/chenjiaxin/zh_CN/index.htm">[Google Scholar]</a> 
  <a href="https://github.com/Dr-Jiaxin-Chen">[GitHub]<a href="http://shi.buaa.edu.cn/chenjiaxin/en/index.htm">[English Page]</a></p>
</td></tr></table>
<h2>关于我</h2>
<p>
陈佳鑫，男，博士。先后于2009年、2012年和2017年获北航应用数学专业学士学位、数学专业硕士学位及计算机应用技术专业博士学位。2017至2018年间在纽约大学阿布扎比分校从事博士后研究工作。2018至2021年在起源人工智能研究院担任研究科学家，现为北京航空航天大学计算机学院副教授/硕士生导师。目前主要研究方向包括计算机视觉（智能视频分析、图像处理、三维视觉分析等）、机器学习（度量学习、哈希学习等）。作为核心成员参与多项国家自然科学基金项目、港澳台合作项目等。相关研究成果已发表论文20余篇，包括IEEE Transactions on Image Processing (TIP)、IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)等顶级期刊；CVPR、ICCV、ECCV等计算机视觉领域顶级会议。</p>
<p>研究方向: 计算机视觉；模式识别；机器学习。</p>
<h2>学术论文</h2>
<h3>会议</h3>
<h4>2022</h4>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.03569">Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement</a> <br />
B. Li, <b>J. Chen*(*：通讯作者)</b>, D. Zhang, X. Bao and D. Huang <br />
<i>International Joint Conference on Artificial Intelligence (IJCAI), 2022. <a href="https://arxiv.org/pdf/2205.03569.pdf">[PDF]</a></i></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_CAT-Det_Contrastively_Augmented_Transformer_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.html">CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection</a> <br />
Y. Zhang, <b>J. Chen</b> and D. Huang <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_CAT-Det_Contrastively_Augmented_Transformer_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.html">[PDF]</a></i></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Target-Relevant_Knowledge_Preservation_for_Multi-Source_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection</a> <br />
J. Wu, <b>J. Chen*(*：通讯作者)</b>, M. He, Y. Wang, B. Li, B. Ma, W. Gan, W. Wu, Y. Wang and D. Huang <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Target-Relevant_Knowledge_Preservation_for_Multi-Source_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">[PDF]</a></i></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Entropy-Based_Active_Learning_for_Object_Detection_With_Progressive_Diversity_Constraint_CVPR_2022_paper.pdf">Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint</a> <br />
J. Wu, <b>J. Chen</b> and D. Huang <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Entropy-Based_Active_Learning_for_Object_Detection_With_Progressive_Diversity_Constraint_CVPR_2022_paper.pdf">[PDF]</a></i></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.10415v2.pdf">UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery</a> <br />
Y. Huang, <b>J. Chen</b> and D. Huang <br />
<i>AAAI Conference on Artificial Intelligencen (AAAI), 2022. <a href="https://arxiv.org/pdf/2112.10415v2.pdf">[PDF]</a></i></p>
</li>
<h4>2021</h4>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9711034/">PR-GCN: A Deep Graph Convolutional Network with Point Refinement for 6D Pose Estimation</a> <br />
G. Zhou, H. Wang, <b>J. Chen</b> and D. Huang <br />
<i> IEEE International Conference on Computer Vision (ICCV), 2021. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9711034&tag=1">[PDF]</a></i></p>
</li>
<h4>2020</h4>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9157770">Auto-Encoding Twin-Bottleneck Hashing</a> <br />
Y. Shen#, J. Qin#, <b>J. Chen# (#: 共同一作)</b>, M. Yu, L. Liu, F. Zhu, F. Shen and L. Shao <br />
<i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9157770">[PDF]</a></i></p>
</li>
<li><p><a href="http://dl.acm.org/doi/pdf/10.1145/3394171.3413979">Deep Local Binary Coding for Person Re- Identification by Delving into the Details</a> <br />
<b>J. Chen</b>, J. Qin, Y. Yan, L. Huang, L. Liu, F. Zhu and L. Shao <br />
<i> ACM Multimedia (MM), 2020. <a href="http://dl.acm.org/doi/pdf/10.1145/3394171.3413979">[PDF]</a></i></p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-030-58555-6_7">Learning Attentive and Hierarchical Representations for 3D Shape Recognition</a> <br />
<b>J. Chen</b>, J. Qin, Y. Shen, L. Liu, F. Zhu and L. Shao <br />
<i> European Conference on Computer Vision (ECCV), 2020. <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-58555-6.pdf">[PDF]</a></i></p>
</li>
<div id="footer">
<div id="footer-text">
<br>Page generated 2022-08-01, by <a href="https://dr-jiaxin-chen.github.io/page/">Jiaxin Chen</a>.
</div>
</div>
</div>
</body>
</html>
